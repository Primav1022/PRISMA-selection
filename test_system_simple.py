#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç³»ç»Ÿæµ‹è¯• - ä½¿ç”¨å°æ¨¡å‹æµ‹è¯•åˆ†æåŠŸèƒ½
=====================================

è¿™ä¸ªè„šæœ¬ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹æ¥æµ‹è¯•è®ºæ–‡åˆ†æç³»ç»Ÿçš„åŸºæœ¬åŠŸèƒ½ï¼Œ
é¿å…ä¸‹è½½å¤§å‹ Phi-3 æ¨¡å‹çš„ç½‘ç»œé—®é¢˜ã€‚
"""

import os
import sys
import pandas as pd
import torch
from pathlib import Path

# è®¾ç½®ç¯å¢ƒå˜é‡ä½¿ç”¨é•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

def test_basic_analysis():
    """ä½¿ç”¨å°æ¨¡å‹æµ‹è¯•åŸºæœ¬åˆ†æåŠŸèƒ½"""
    print("ğŸ”¬ åŸºæœ¬åˆ†æåŠŸèƒ½æµ‹è¯•")
    print("-" * 40)
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        # ä½¿ç”¨è¾ƒå°çš„ä¸­æ–‡æ¨¡å‹è¿›è¡Œæµ‹è¯•
        model_name = "uer/gpt2-chinese-cluecorpussmall"
        print(f"ğŸ“¥ åŠ è½½æ¨¡å‹: {model_name}")
        
        # åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        
        # è®¾ç½® pad_token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œä½¿ç”¨è®¾å¤‡: {device}")
        print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
        
        # æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ
        test_text = "äººå·¥æ™ºèƒ½åœ¨å­¦æœ¯ç ”ç©¶ä¸­çš„åº”ç”¨"
        print(f"ğŸ”¤ æµ‹è¯•æ–‡æœ¬: '{test_text}'")
        
        inputs = tokenizer(test_text, return_tensors='pt').to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"âœ¨ ç”Ÿæˆç»“æœ: '{generated_text}'")
        print("âœ… æ–‡æœ¬ç”Ÿæˆæµ‹è¯•æˆåŠŸï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_pdf_processing():
    """æµ‹è¯• PDF å¤„ç†åŠŸèƒ½"""
    print("\nğŸ“„ PDF å¤„ç†åŠŸèƒ½æµ‹è¯•")
    print("-" * 40)
    
    try:
        # å¯¼å…¥ PDF å¤„ç†æ¨¡å—
        import importlib.util
        spec = importlib.util.spec_from_file_location("analysis", "5_intelligent_with_phi3.py")
        analysis_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(analysis_module)
        
        # åˆ›å»º PDF å¤„ç†å™¨
        pdf_processor = analysis_module.PDFProcessor()
        print("âœ… PDF å¤„ç†å™¨åˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•æ–‡æœ¬é¢„å¤„ç†
        test_text = """
        This is a test document with multiple    spaces
        and
        
        multiple
        line breaks.
        
        It should be cleaned up by the preprocessing function.
        """
        
        processed_text = pdf_processor.preprocess_text(test_text)
        print("ğŸ”§ æ–‡æœ¬é¢„å¤„ç†æµ‹è¯•:")
        print(f"åŸå§‹é•¿åº¦: {len(test_text)}")
        print(f"å¤„ç†åé•¿åº¦: {len(processed_text)}")
        print(f"å¤„ç†ç»“æœ: {processed_text[:100]}...")
        print("âœ… æ–‡æœ¬é¢„å¤„ç†æµ‹è¯•æˆåŠŸï¼")
        
        return True
        
    except Exception as e:
        print(f"âŒ PDF å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_csv_reading():
    """æµ‹è¯• CSV æ–‡ä»¶è¯»å–"""
    print("\nğŸ“Š CSV æ–‡ä»¶è¯»å–æµ‹è¯•")
    print("-" * 40)
    
    csv_file = "output/merged_cleaned_results_intelligent_cleaned.csv"
    
    if not os.path.exists(csv_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {csv_file}")
        print("è¯·ç¡®ä¿å·²è¿è¡Œå‰é¢çš„æ•°æ®æ”¶é›†æ­¥éª¤")
        return False
    
    try:
        df = pd.read_csv(csv_file, encoding='utf-8')
        print(f"âœ… æˆåŠŸè¯»å– CSV æ–‡ä»¶")
        print(f"ğŸ“ˆ è®ºæ–‡æ•°é‡: {len(df)}")
        print(f"ğŸ“‹ åˆ—å: {list(df.columns)}")
        
        # æ˜¾ç¤ºå‰å‡ æ¡è®°å½•çš„åŸºæœ¬ä¿¡æ¯
        print("\nğŸ“ å‰ 3 æ¡è®°å½•:")
        for i, (idx, row) in enumerate(df.head(3).iterrows()):
            print(f"\n  è®ºæ–‡ {i+1}:")
            print(f"    æ ‡é¢˜: {row['title'][:60]}...")
            print(f"    ä½œè€…: {row['authors']}")
            print(f"    å¹´ä»½: {row['year']}")
            print(f"    å¼•ç”¨æ•°: {row['citations']}")
        
        return True
        
    except Exception as e:
        print(f"âŒ CSV è¯»å–å¤±è´¥: {e}")
        return False

def create_simple_demo():
    """åˆ›å»ºç®€å•æ¼”ç¤º"""
    print("\nğŸ¯ åˆ›å»ºç®€å•æ¼”ç¤º")
    print("-" * 40)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    sample_data = {
        'title': [
            'Artificial Intelligence in Healthcare: A Comprehensive Review',
            'Machine Learning Applications in Academic Research',
            'Deep Learning for Natural Language Processing'
        ],
        'authors': [
            'Zhang, L.; Wang, M.; Li, H.',
            'Smith, J.; Brown, A.',
            'Johnson, K.; Davis, R.'
        ],
        'year': [2023, 2022, 2024],
        'citations': [45, 32, 18],
        'abstract': [
            'This paper presents a comprehensive review of artificial intelligence applications in healthcare, focusing on machine learning algorithms and their clinical implementations.',
            'We explore various machine learning techniques used in academic research, including data mining, predictive modeling, and statistical analysis methods.',
            'This study investigates deep learning approaches for natural language processing tasks, with emphasis on transformer architectures and attention mechanisms.'
        ]
    }
    
    # ä¿å­˜ä¸º CSV
    df = pd.DataFrame(sample_data)
    demo_file = "output/demo_papers.csv"
    df.to_csv(demo_file, index=False, encoding='utf-8-sig')
    print(f"âœ… åˆ›å»ºæ¼”ç¤ºæ•°æ®: {demo_file}")
    
    return demo_file

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ç®€åŒ–ç³»ç»Ÿæµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    Path("output").mkdir(exist_ok=True)
    Path("log").mkdir(exist_ok=True)
    
    results = []
    
    # æµ‹è¯•åŸºæœ¬åˆ†æåŠŸèƒ½
    results.append(("GPU/æ¨¡å‹æµ‹è¯•", test_basic_analysis()))
    
    # æµ‹è¯• PDF å¤„ç†
    results.append(("PDFå¤„ç†æµ‹è¯•", test_pdf_processing()))
    
    # æµ‹è¯• CSV è¯»å–
    results.append(("CSVè¯»å–æµ‹è¯•", test_csv_reading()))
    
    # åˆ›å»ºæ¼”ç¤ºæ•°æ®
    demo_file = create_simple_demo()
    results.append(("æ¼”ç¤ºæ•°æ®åˆ›å»º", demo_file is not None))
    
    # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
    print("\n" + "=" * 50)
    print("ğŸ“‹ æµ‹è¯•ç»“æœæ€»ç»“:")
    print("-" * 50)
    
    passed = 0
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name:15} : {status}")
        if result:
            passed += 1
    
    print(f"\nğŸ“Š æ€»ä½“ç»“æœ: {passed}/{len(results)} é¡¹æµ‹è¯•é€šè¿‡")
    
    if passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç³»ç»ŸåŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("1. å°è¯•ä½¿ç”¨è¾ƒå°çš„ä¸­æ–‡æ¨¡å‹æ›¿ä»£ Phi-3")
        print("2. æˆ–è€…æ‰‹åŠ¨ä¸‹è½½ Phi-3 æ¨¡å‹åˆ°æœ¬åœ°")
        print("3. ä½¿ç”¨æ¼”ç¤ºæ•°æ®æµ‹è¯•å®Œæ•´æµç¨‹")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³é…ç½®")
    
    print("\nğŸ”§ å¦‚æœè¦ä½¿ç”¨ Phi-3 æ¨¡å‹ï¼Œå»ºè®®:")
    print("1. é…ç½®ç½‘ç»œä»£ç†æˆ–ä½¿ç”¨é•œåƒç«™ç‚¹")
    print("2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
    print("3. æˆ–ä½¿ç”¨å…¶ä»–å…¼å®¹çš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹")

if __name__ == "__main__":
    main()
