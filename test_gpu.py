#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU åŠŸèƒ½æµ‹è¯•è„šæœ¬
==============

æµ‹è¯• PyTorch GPU åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import time

def test_gpu_basic():
    """åŸºç¡€ GPU æµ‹è¯•"""
    print("ğŸ” åŸºç¡€ GPU æµ‹è¯•")
    print("-" * 40)
    
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPU æ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰ GPU: {torch.cuda.current_device()}")
        print(f"GPU åç§°: {torch.cuda.get_device_name(0)}")
        
        # æ˜¾ç¤º GPU å†…å­˜ä¿¡æ¯
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU å†…å­˜: {gpu_memory:.2f} GB")
        
        # æ˜¾ç¤ºå½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        cached = torch.cuda.memory_reserved(0) / 1024**3
        print(f"å·²åˆ†é…å†…å­˜: {allocated:.2f} GB")
        print(f"ç¼“å­˜å†…å­˜: {cached:.2f} GB")
        
        return True
    else:
        print("âŒ CUDA ä¸å¯ç”¨")
        return False

def test_gpu_computation():
    """GPU è®¡ç®—æµ‹è¯•"""
    print("\nğŸ§® GPU è®¡ç®—æµ‹è¯•")
    print("-" * 40)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡è®¡ç®—æµ‹è¯•")
        return
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    size = 1000
    print(f"åˆ›å»º {size}x{size} çŸ©é˜µè¿›è¡Œæµ‹è¯•...")
    
    # CPU è®¡ç®—
    print("â±ï¸  CPU è®¡ç®—æµ‹è¯•...")
    a_cpu = torch.randn(size, size)
    b_cpu = torch.randn(size, size)
    
    start_time = time.time()
    c_cpu = torch.mm(a_cpu, b_cpu)
    cpu_time = time.time() - start_time
    print(f"CPU è®¡ç®—æ—¶é—´: {cpu_time:.4f} ç§’")
    
    # GPU è®¡ç®—
    print("ğŸš€ GPU è®¡ç®—æµ‹è¯•...")
    device = torch.device('cuda')
    a_gpu = torch.randn(size, size, device=device)
    b_gpu = torch.randn(size, size, device=device)
    
    # é¢„çƒ­ GPU
    torch.mm(a_gpu, b_gpu)
    torch.cuda.synchronize()
    
    start_time = time.time()
    c_gpu = torch.mm(a_gpu, b_gpu)
    torch.cuda.synchronize()
    gpu_time = time.time() - start_time
    print(f"GPU è®¡ç®—æ—¶é—´: {gpu_time:.4f} ç§’")
    
    # è®¡ç®—åŠ é€Ÿæ¯”
    if gpu_time > 0:
        speedup = cpu_time / gpu_time
        print(f"ğŸƒâ€â™‚ï¸ GPU åŠ é€Ÿæ¯”: {speedup:.2f}x")
    
    # éªŒè¯ç»“æœæ­£ç¡®æ€§
    c_gpu_cpu = c_gpu.cpu()
    max_diff = torch.max(torch.abs(c_cpu - c_gpu_cpu)).item()
    print(f"âœ… è®¡ç®—ç»“æœå·®å¼‚: {max_diff:.2e} (åº”è¯¥æ¥è¿‘0)")

def test_transformers_gpu():
    """æµ‹è¯• transformers åº“çš„ GPU æ”¯æŒ"""
    print("\nğŸ¤– Transformers GPU æµ‹è¯•")
    print("-" * 40)
    
    try:
        from transformers import AutoTokenizer
        import torch
        
        if not torch.cuda.is_available():
            print("âŒ CUDA ä¸å¯ç”¨ï¼Œè·³è¿‡ transformers GPU æµ‹è¯•")
            return
        
        print("âœ… transformers åº“å¯¼å…¥æˆåŠŸ")
        print("âœ… GPU è®¾å¤‡å¯ç”¨")
        print("ğŸ¯ å»ºè®®ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•ï¼Œå¦‚ 'gpt2' æˆ– 'distilbert-base-uncased'")
        
        # æµ‹è¯•ä¸€ä¸ªå°æ¨¡å‹ï¼ˆå¦‚æœç”¨æˆ·æƒ³è¦çš„è¯ï¼‰
        test_small_model = input("\næ˜¯å¦æµ‹è¯•å°æ¨¡å‹ GPT-2ï¼Ÿ(y/n): ").lower().strip()
        if test_small_model == 'y':
            print("ğŸ“¥ åŠ è½½ GPT-2 æ¨¡å‹...")
            from transformers import AutoModelForCausalLM
            
            tokenizer = AutoTokenizer.from_pretrained('gpt2')
            model = AutoModelForCausalLM.from_pretrained('gpt2')
            
            # ç§»åŠ¨åˆ° GPU
            device = torch.device('cuda')
            model = model.to(device)
            
            print("âœ… æ¨¡å‹å·²åŠ è½½åˆ° GPU")
            print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
            
            # ç®€å•æµ‹è¯•
            text = "The future of AI is"
            inputs = tokenizer(text, return_tensors='pt').to(device)
            
            print(f"ğŸ”¤ æµ‹è¯•æ–‡æœ¬: '{text}'")
            print("ğŸ¯ ç”Ÿæˆä¸­...")
            
            with torch.no_grad():
                outputs = model.generate(
                    **inputs, 
                    max_new_tokens=20, 
                    do_sample=True, 
                    temperature=0.7
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"âœ¨ ç”Ÿæˆç»“æœ: '{generated_text}'")
            print("âœ… GPU æ¨¡å‹æµ‹è¯•æˆåŠŸï¼")
        
    except ImportError:
        print("âŒ transformers åº“æœªå®‰è£…")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ GPU åŠŸèƒ½æµ‹è¯•å¼€å§‹")
    print("=" * 50)
    
    # åŸºç¡€ GPU æµ‹è¯•
    gpu_available = test_gpu_basic()
    
    if gpu_available:
        # è®¡ç®—æ€§èƒ½æµ‹è¯•
        test_gpu_computation()
        
        # Transformers GPU æµ‹è¯•
        test_transformers_gpu()
    
    print("\n" + "=" * 50)
    print("ğŸ æµ‹è¯•å®Œæˆï¼")
    
    if gpu_available:
        print("âœ… æ‚¨çš„ GPU è®¾ç½®æ­£å¸¸ï¼Œå¯ä»¥è¿è¡Œ Phi-3 åˆ†æç³»ç»Ÿ")
        print("ğŸ’¡ å¦‚æœæ¨¡å‹ä¸‹è½½é‡åˆ°é—®é¢˜ï¼Œå¯ä»¥ï¼š")
        print("   1. ä½¿ç”¨ HuggingFace é•œåƒ")
        print("   2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
        print("   3. ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹è¿›è¡Œæµ‹è¯•")
    else:
        print("âš ï¸  GPU ä¸å¯ç”¨ï¼Œç³»ç»Ÿå°†ä½¿ç”¨ CPU æ¨¡å¼ï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")

if __name__ == "__main__":
    main()
