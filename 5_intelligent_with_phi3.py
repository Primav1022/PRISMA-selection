#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PDF è®ºæ–‡æ™ºèƒ½åˆ†æç³»ç»Ÿ - ä½¿ç”¨ Phi-3-mini æ¨¡å‹
==================================================

è¿™ä¸ªè„šæœ¬ç”¨äºï¼š
1. ä» CSV æ–‡ä»¶ä¸­è¯»å–è®ºæ–‡ä¿¡æ¯
2. ä¸‹è½½æˆ–å¤„ç† PDF æ–‡ä»¶
3. ä½¿ç”¨ Phi-3-mini æ¨¡å‹è¿›è¡Œæ–‡æœ¬æ€»ç»“å’Œåˆ†æ
4. ç”Ÿæˆå¯è§†åŒ–ç»“æœå’ŒæŠ¥å‘Š

ä½œè€…: 
æ—¥æœŸ: 2024
ç‰ˆæœ¬: 1.0
"""

import os
import sys
import logging
import pandas as pd
import numpy as np
import requests
import fitz  # PyMuPDF
import pdfplumber
import json
import re
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from datetime import datetime
from urllib.parse import urlparse
import time

# AI æ¨¡å‹ç›¸å…³
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# å¯è§†åŒ–ç›¸å…³
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# è¿›åº¦æ¡
from tqdm.auto import tqdm

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class PDFProcessor:
    """PDF æ–‡ä»¶å¤„ç†å™¨"""
    
    def __init__(self, download_dir: str = "pdfs"):
        """
        åˆå§‹åŒ– PDF å¤„ç†å™¨
        
        Args:
            download_dir: PDF æ–‡ä»¶ä¸‹è½½ç›®å½•
        """
        self.download_dir = Path(download_dir)
        self.download_dir.mkdir(exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('log/pdf_processing.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def download_pdf(self, url: str, title: str) -> Optional[str]:
        """
        ä¸‹è½½ PDF æ–‡ä»¶
        
        Args:
            url: PDF æ–‡ä»¶ URL
            title: è®ºæ–‡æ ‡é¢˜ï¼ˆç”¨äºæ–‡ä»¶å‘½åï¼‰
            
        Returns:
            ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            # æ¸…ç†æ–‡ä»¶å
            safe_title = re.sub(r'[<>:"/\\|?*]', '', title)[:100]
            filename = f"{safe_title}.pdf"
            filepath = self.download_dir / filename
            
            # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
            if filepath.exists():
                self.logger.info(f"æ–‡ä»¶å·²å­˜åœ¨: {filename}")
                return str(filepath)
            
            # ä¸‹è½½æ–‡ä»¶
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            
            # æ£€æŸ¥æ˜¯å¦ä¸º PDF æ–‡ä»¶
            if 'application/pdf' not in response.headers.get('content-type', ''):
                self.logger.warning(f"URL ä¸æ˜¯ PDF æ–‡ä»¶: {url}")
                return None
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, 'wb') as f:
                f.write(response.content)
            
            self.logger.info(f"æˆåŠŸä¸‹è½½: {filename}")
            return str(filepath)
            
        except Exception as e:
            self.logger.error(f"ä¸‹è½½å¤±è´¥ {url}: {str(e)}")
            return None
    
    def extract_text_from_pdf(self, pdf_path: str) -> Optional[str]:
        """
        ä» PDF æ–‡ä»¶ä¸­æå–æ–‡æœ¬
        
        Args:
            pdf_path: PDF æ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            text_content = ""
            
            # æ–¹æ³•1: ä½¿ç”¨ PyMuPDF
            try:
                with fitz.open(pdf_path) as doc:
                    for page in doc:
                        text_content += page.get_text()
                        
                if len(text_content.strip()) > 100:  # å¦‚æœæå–åˆ°è¶³å¤Ÿçš„æ–‡æœ¬
                    return text_content
            except Exception as e:
                self.logger.warning(f"PyMuPDF æå–å¤±è´¥: {str(e)}")
            
            # æ–¹æ³•2: ä½¿ç”¨ pdfplumberï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
            try:
                with pdfplumber.open(pdf_path) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text_content += page_text + "\n"
                            
                if len(text_content.strip()) > 100:
                    return text_content
            except Exception as e:
                self.logger.warning(f"pdfplumber æå–å¤±è´¥: {str(e)}")
            
            return text_content if text_content.strip() else None
            
        except Exception as e:
            self.logger.error(f"æ–‡æœ¬æå–å¤±è´¥ {pdf_path}: {str(e)}")
            return None
    
    def preprocess_text(self, text: str, max_length: int = 4000) -> str:
        """
        é¢„å¤„ç†æ–‡æœ¬å†…å®¹
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦é™åˆ¶
            
        Returns:
            é¢„å¤„ç†åçš„æ–‡æœ¬
        """
        if not text:
            return ""
        
        # æ¸…ç†æ–‡æœ¬
        text = re.sub(r'\s+', ' ', text)  # åˆå¹¶å¤šä¸ªç©ºç™½å­—ç¬¦
        text = re.sub(r'\n+', '\n', text)  # åˆå¹¶å¤šä¸ªæ¢è¡Œç¬¦
        text = text.strip()
        
        # æˆªå–å‰é¢éƒ¨åˆ†ï¼ˆé€šå¸¸åŒ…å«æ‘˜è¦å’Œä¸»è¦å†…å®¹ï¼‰
        if len(text) > max_length:
            text = text[:max_length] + "..."
        
        return text


class Phi3Analyzer:
    """Phi-3 æ¨¡å‹åˆ†æå™¨"""
    
    def __init__(self, model_name: str = "microsoft/Phi-3-mini-4k-instruct"):
        """
        åˆå§‹åŒ– Phi-3 åˆ†æå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        self._load_model()
    
    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨"""
        try:
            self.logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")
            self.logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
            
            # å°è¯•å¤šæ¬¡ä¸‹è½½ï¼Œå¤„ç†ç½‘ç»œé—®é¢˜
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    # åŠ è½½åˆ†è¯å™¨
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        self.model_name,
                        trust_remote_code=True,
                        padding_side="left",
                        force_download=(attempt > 0)  # ç¬¬ä¸€æ¬¡å¤±è´¥åå¼ºåˆ¶é‡æ–°ä¸‹è½½
                    )
                    
                    # è®¾ç½® pad_token
                    if self.tokenizer.pad_token is None:
                        self.tokenizer.pad_token = self.tokenizer.eos_token
                    
                    # åŠ è½½æ¨¡å‹
                    self.model = AutoModelForCausalLM.from_pretrained(
                        self.model_name,
                        torch_dtype=torch.float16 if self.device.type == "cuda" else torch.float32,
                        device_map="auto" if self.device.type == "cuda" else None,
                        trust_remote_code=True,
                        force_download=(attempt > 0),  # ç¬¬ä¸€æ¬¡å¤±è´¥åå¼ºåˆ¶é‡æ–°ä¸‹è½½
                        attn_implementation="eager"  # ä½¿ç”¨eager attentionï¼Œé¿å…flash_attnè­¦å‘Š
                    )
                    
                    if self.device.type == "cpu":
                        self.model = self.model.to(self.device)
                    
                    self.logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")
                    return  # æˆåŠŸåé€€å‡ºé‡è¯•å¾ªç¯
                    
                except Exception as e:
                    self.logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {str(e)}")
                    if attempt == max_retries - 1:
                        raise  # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
                    
                    # æ¸…ç†å¯èƒ½æŸåçš„ç¼“å­˜æ–‡ä»¶
                    import shutil
                    from pathlib import Path
                    cache_dir = Path.home() / ".cache" / "huggingface" / "hub" / f"models--{self.model_name.replace('/', '--')}"
                    if cache_dir.exists():
                        self.logger.info("æ¸…ç†å¯èƒ½æŸåçš„ç¼“å­˜æ–‡ä»¶...")
                        shutil.rmtree(cache_dir, ignore_errors=True)
                    
                    time.sleep(2)  # ç­‰å¾…2ç§’åé‡è¯•
            
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.logger.info("å»ºè®®è§£å†³æ–¹æ¡ˆ:")
            self.logger.info("1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
            self.logger.info("2. è®¾ç½® HuggingFace é•œåƒ: export HF_ENDPOINT=https://hf-mirror.com")
            self.logger.info("3. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°")
            raise
    
    def generate_summary(self, text: str, max_new_tokens: int = 256) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬æ‘˜è¦
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            
        Returns:
            ç”Ÿæˆçš„æ‘˜è¦
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = f"""è¯·ä¸ºä»¥ä¸‹å­¦æœ¯è®ºæ–‡å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€æ´çš„ä¸­æ–‡æ‘˜è¦ï¼ŒåŒ…å«ä»¥ä¸‹è¦ç‚¹ï¼š
1. ç ”ç©¶ä¸»é¢˜å’Œç›®æ ‡
2. ä¸»è¦æ–¹æ³•æˆ–æŠ€æœ¯
3. å…³é”®å‘ç°æˆ–è´¡çŒ®
4. åº”ç”¨ä»·å€¼æˆ–æ„ä¹‰

è®ºæ–‡å†…å®¹ï¼š
{text}

æ‘˜è¦ï¼š"""

            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=3500,  # ä¸ºç”Ÿæˆç•™å‡ºç©ºé—´
                padding=True
            ).to(self.device)
            
            # ç”Ÿæˆæ‘˜è¦
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.9,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            # è§£ç è¾“å‡º
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            self.logger.error(f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}")
            return f"æ‘˜è¦ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def extract_keywords(self, text: str, max_new_tokens: int = 100) -> List[str]:
        """
        æå–å…³é”®è¯
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_new_tokens: æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        try:
            prompt = f"""è¯·ä»ä»¥ä¸‹å­¦æœ¯è®ºæ–‡å†…å®¹ä¸­æå–5-10ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œç”¨é€—å·åˆ†éš”ï¼š

è®ºæ–‡å†…å®¹ï¼š
{text[:2000]}

å…³é”®è¯ï¼š"""

            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2500,
                padding=True
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.5,
                    top_p=0.8,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            # è§£æå…³é”®è¯
            keywords = [kw.strip() for kw in generated_text.split(',')]
            return [kw for kw in keywords if kw and len(kw) > 1][:10]
            
        except Exception as e:
            self.logger.error(f"å…³é”®è¯æå–å¤±è´¥: {str(e)}")
            return []


def main():
    """ä¸»å‡½æ•°"""
    try:
        print("ğŸš€ å¯åŠ¨è®ºæ–‡æ™ºèƒ½åˆ†æç³»ç»Ÿ")
        print("ğŸ“‹ åŠŸèƒ½ï¼š")
        print("  1. PDF æ–‡æœ¬æå–")
        print("  2. Phi-3-mini æ¨¡å‹åˆ†æ")
        print("  3. æ™ºèƒ½æ‘˜è¦ç”Ÿæˆ")
        print("  4. å…³é”®è¯æå–")
        print("-" * 50)
        
        # ç¤ºä¾‹ç”¨æ³•
        csv_file = "output/merged_cleaned_results_intelligent_cleaned.csv"
        
        if not os.path.exists(csv_file):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ {csv_file}")
            print("è¯·ç¡®ä¿ CSV æ–‡ä»¶å­˜åœ¨")
            return
        
        # è¯»å–æ•°æ®
        df = pd.read_csv(csv_file, encoding='utf-8')
        print(f"ğŸ“Š æˆåŠŸåŠ è½½ {len(df)} æ¡è®ºæ–‡è®°å½•")
        
        # åˆå§‹åŒ–å¤„ç†å™¨
        pdf_processor = PDFProcessor()
        phi3_analyzer = Phi3Analyzer()
        
        print("\nâœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        print("ğŸ’¡ æç¤ºï¼šè¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹è„šæœ¬ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹å’Œæ‰©å±•åŠŸèƒ½ã€‚")
        
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
